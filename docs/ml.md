## Deeplearing book 

## Chapter 6
  * Rectified Linear Activation Function
    - This function is the default activation function recommended for use with most feedforward neural networks. 
      * Applying this function to the output of a linear transformation yields a nonlinear transformation. However, 
        the function remains very close to linear, in the sense that is a piecewise linear function with two linear
        pieces. Because rectified linear units are nearly linear, they preserve many of the properties that make 
        linear models easy to optimize with gradient-based methods. They also preserve many of the properties that
        make linear models generalize well. 
      * A common principle throughout computer science is that we can build complicated systems from minimal components.
        Much as a Turing machines-s memory needes only to be able to store 0 and 1 states, we can build a __universal function 
        approximator__ from rectified linear functions.
  * Most modern neural networks are trained using maximum likehood. This means that the cost function is simply the negative
    log-likelihood, equivalently described as the cross-entropy between the training data and the model distribution. 
    - Deriving the cost function from maximum likelihood is that is removes the burden of designing cost functions for each model.
      Specifying a model p(y|x) automatically determines a cost function LOGp(y|x).

